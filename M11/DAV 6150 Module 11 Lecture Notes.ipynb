{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 11: Decision Trees & Random Forests\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "- __Decision trees__ are __non-parametric supervised learning algorithms__ that are widely used for classification problems. By \"non parametric\" we mean that we need not make any assumptions regarding the \"shape\" of the data we are attempting to model (e.g., we don't require \"normality\" or any other specific type of data distribution).\n",
    "\n",
    "\n",
    "- Decision trees are __used for non-linear decision making__ and are composed of __\"nodes\"__ and __\"edges\"__\n",
    "\n",
    "\n",
    "- The __“root node”__ represents the top of the tree\n",
    "\n",
    "\n",
    "- __“Leaf nodes”__ (a.k.a. “terminal nodes”) represent the locations within a tree where a data value is assigned to a previously unseen observation \n",
    "\n",
    "\n",
    "- All nodes located above the leaf nodes (referred to as __\"non-leaf nodes\"__) can be thought of as points at which we must answer an __“if-then-else”__ question\n",
    "\n",
    "\n",
    "- __Edges__ represent the viable answers to the questions we must answer at the non-leaf nodes, with each edge forming a pathway between a relatively higher-level node and a node located at the next lowest level of the tree. \n",
    "\n",
    "\n",
    "- Most frequently used for __classification__ problems, but can also be used for __regression analysis__\n",
    "\n",
    "\n",
    "### How They Work\n",
    "\n",
    "- Decision trees separate data via a series of if-then-else questions that are applied to each observation within a data set, with the first such question serving as the “root node” of the tree. The answers to the if-then-else questions then serve as pathways (or “edges”) to additional nodes within the tree wherein we either ask additional if-then-else questions (non-leaf nodes) or we assign a data value to the given observation. The most common algorithm used for constructing a decision tree is __ID3__ (\"Iterative Dichotomiser 3\"), which applies a top-down greedy search approach to construct a tree.\n",
    "\n",
    "\n",
    "- In __ID3__, we start by selecting the independent variable that appears to offer the largest amount of __information gain__, a statistical property that __measures how well a given attribute separates the training examples according to their target classification__. The attribute offering the largest amount of information gain is used as the __root node__ of the decision tree. \n",
    "\n",
    "\n",
    "- An attribute having __low information gain__ splits the data relatively evenly among the possible target classifications while an attribute having __high information gain__ split the data relatively unevenly among the possible target classifications.\n",
    "\n",
    "\n",
    "- To calculate __information gain__ for a particular attribute, we must first calculate the __entropy__ value for the attribute. Entropy __is a measure of uncertainty__ regarding the possible data values that can be observed for a particular attribute. If an attribute has only a single possible data value, its entropy is $0$. If the possible values of an attribute are equivalently distributed, its entropy is $1$. If an attribute has any imbalance amongst the distribution of its data values, it will have an entropy value somewhere between $0$ and $1$.\n",
    "\n",
    "\n",
    "- __Information gain__ reflects the amount of entropy that has been __removed__ after a dataset is split on an attribute. \n",
    "\n",
    "\n",
    "- So as we proceed with the creation of nodes and edges from the root node, we seek to __maximize the amount of information gain__ at each sub-node (i.e., decrease the amount of remaining entropy) until we achieve an information gain value of $0$ at each of the lowest level nodes of the tree.\n",
    "\n",
    "\n",
    "- A detailed explanation of how __entropy__ is calculated can be accessed here: https://www.saedsayad.com/decision_tree.htm\n",
    "\n",
    "\n",
    "\n",
    "- An example of how decision trees work (including a detailed explanation of both information gain and entropy) from the assigned readings: https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Easy to implement, understand and interpret\n",
    "\n",
    "\n",
    "- Can handle both categorical and numerical data. \n",
    "\n",
    "\n",
    "- Decision trees implicitly perform variable screening or feature selection\n",
    "\n",
    "\n",
    "- Not very susceptible to influence of outliers\n",
    "\n",
    "\n",
    "- New attributes / features can be easily added to a decision tree model \n",
    "\n",
    "\n",
    "- Performs well on large data sets\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Decision trees are prone to __overfitting__. Overfitting can be alleviated by either limiting the depth of the tree during model building (i.e., prevent it from perfectly classifying every training observation) or via the use of __pruning__, wherein we allow the tree to perfectly classify all of the training data and then subsequently remove some number of levels from the tree to prevent it from perfectly classifying all of the training data. When pruning, we continue to remove leaf nodes from a tree as long as the pruned version of the tree continues to outperform the original tree when applied to identical training data sets.\n",
    "\n",
    "\n",
    "- Decision trees can be unstable because small variations in the data might result in a completely different tree being generated.\n",
    "\n",
    "\n",
    "- Can be a challenge to determine how many levels to permit or prune: need to use an empirical approach\n",
    "\n",
    "\n",
    "- Prone to bias if target classification values are dominated by a single value\n",
    "\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "- Many kinds of classification problems\n",
    "\n",
    "\n",
    "- Data Mining\n",
    "\n",
    "\n",
    "- Used as components of __ensemble models__ (e.g., __random forests__)\n",
    "\n",
    "\n",
    "\n",
    "### How to Implement a Decision Tree Classifier in Python\n",
    "\n",
    "The __scikit-learn__ library includes a pre-built ID3 decision tree classifier: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "\n",
    "Examples of how to use a decision tree classifier for text classification from the Module 11 assigned readings: \n",
    "\n",
    "- https://github.com/ageron/handson-ml2/blob/master/06_decision_trees.ipynb\n",
    "\n",
    "\n",
    "- https://www.hackerearth.com/practice/machine-learning/machine-learning-algorithms/ml-decision-tree/tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "- __Random forests__ are __ensemble models__ that make use of the output of a variety of different decision tree models to assign a data value to an observation. \n",
    "\n",
    "\n",
    "- The use of numerous individual decision trees is what has led to the use of the term __forest__. The individual trees are created via the use of __random sampling__ of data observations as well as the __random sampling__ of explanatory variables / features when splitting decision tree nodes into sub-nodes.\n",
    "\n",
    "\n",
    "- Random forest models perform best when the attributes used to train the underlying decision trees are as independent from one another as possible \n",
    "\n",
    "\n",
    "- Most frequently used for __classification__ problems, but can also be used for __regression analysis__\n",
    "\n",
    "\n",
    "### How They Work\n",
    "\n",
    "- Each decision tree is trained on a different random subset of the explanatory variables contained within a data set. \n",
    "\n",
    "\n",
    "- For each previously unseen observation the random forest model then either “averages” the output of all of the individual component decision trees (e.g., when the response variable is numeric) or selects the most frequently occurring classification value (e.g., when the response variable is a categorical) to assign a data value to the observation. \n",
    "\n",
    "\n",
    "In general, this ensemble approach encourages us to __continue to add decision trees to the ensemble as long as the individual trees have a better than 50% chance of assigning a valid data value to an observation__. \n",
    "\n",
    "\n",
    "#### Bootstrap Aggregation\n",
    "\n",
    "The use of __randomly sampled subsets to train each individual decision tree__ and then __averaging the resultant predictions__ is known as __“bootstrap aggregation”__ (a.k.a. __\"bagging\"__). This approach results in the output of a random forest model having a __lower variance__ than that of its individual component decision trees __without increasing model bias__. \n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- Random forests implicitly perform variable screening or feature selection\n",
    "\n",
    "\n",
    "- Not prone to failure due to missing data values\n",
    "\n",
    "\n",
    "- Not prone to overfitting\n",
    "\n",
    "\n",
    "- Capable of handling large data sets that have many features (i.e., high dimensionality)\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- Much more complex than individual decision trees => much more difficult to interpret\n",
    "\n",
    "\n",
    "- If the explanatory variables used to train the underlying decision trees are not relatively independent of one another, the resulting random forest model can suffer from significant performance issues\n",
    "\n",
    "\n",
    "- Model developers have very little control over how the model behaves. As with decision tree, need to use an empirical approach when developing a model (i.e., construct many different models using varying random seeds + hyperparameters and then select the \"best\" model)\n",
    "\n",
    "\n",
    "### Common Applications\n",
    "\n",
    "Like decision trees, random forests are widely used for __classification problems__ throughout many fields, including healthcare, finance, manufacturing and logistics, amongst others.\n",
    "\n",
    "\n",
    "### How to Implement a Random Forest Classifier in Python\n",
    "\n",
    "The __scikit-learn__ library includes a pre-built random forest classifier: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "\n",
    "Examples of how to use a random forest classifier for text classification from the Module 11 assigned readings: \n",
    "\n",
    "- https://github.com/ageron/handson-ml2/blob/master/07_ensemble_learning_and_random_forests.ipynb\n",
    "\n",
    "\n",
    "- https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 11 Assignment Guidelines / Requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
