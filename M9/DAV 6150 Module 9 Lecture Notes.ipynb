{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# DAV 6150 Module 9: Clustering\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 8 Assignment Review\n",
    "\n",
    "#### ALWAYS assess the performance of your models via a variety of performance metrics\n",
    "\n",
    "Relying solely on an accuracy or F1 score when comparing models is __extremely__ unusual in \"real world\" predictive modeling work. Models should __always__ be compared via a variety of metrics to help us enhance our understanding of the true performance of the model. (NOTE: this holds true even if an unbalanced response variable has been balanced via an algorithm). \n",
    "\n",
    "For example, if we have two models, both of which have high accuracy, but one has a higher recall score while the other has a higher specificity score, if we rely solely on the accuracy score we will fail to recognize that one of the models is better suited to accurately predicting true positives (the model with higher recall) while the other is better suited to accurately predicting true negatives (the model with higher specificity).\n",
    "\n",
    "Also, classifiers can be compared on the basis of AIC, BIC (which incorporates a model \"complexity\" penalty), and log-likelihood scores, each of which (as explained in __Module 5__) are different ways of measuring the \"goodness of fit\" of a model.\n",
    "\n",
    "#### Both KNN and SVM modeling will benefit from the use of \"scaled\" numeric values\n",
    "\n",
    "- Ideally, continous variables should be of similar scales and preferably free of negative data values, so normalization and min-max scaling are two of the most frequently used scaling methods for KNN + SVM modeling.\n",
    "\n",
    "\n",
    "- Categorical variables need to be converted to digits, so convert nominal categorical variables to binary dummy variables and convert ordinal categorical data into ordered cardinal integers.\n",
    "\n",
    "\n",
    "- Dummy variables + ordered cardinal integers derived from ordinal categorical data __DO NOT__ need to be \"scaled\" for use in a KNN or SVM model.\n",
    "\n",
    "#### Use Tables to Summarize Your Model Performance Metrics\n",
    "\n",
    "When comparing various disparate models in your writeups, use of a summarial table is preferred for enabling a side-by-side comparison of the model performance metrics. \n",
    "\n",
    "An example of a table created \"by hand\" in Markdown for comparing classification models (note the inclusion of the number of explanatory variables used in each model):\n",
    "\n",
    "\n",
    "| Metric        | Model 1 | Model 2 | Model 3\n",
    "| ------------- | ------- | ------- | -------\n",
    "| # Indep. Vars |  17     |   13    |   17    \n",
    "| AIC           | 7395.9  | 7511.1  | 7430.4 \n",
    "| Accuracy      | 0.7854  | 0.7863  | 0.7827 \n",
    "| Class.Err.R.  | 0.2145  | 0.2137  | 0.2173 \n",
    "| Precision     | 0.6478  | 0.6594  | 0.6480 \n",
    "| Sensitivity   | 0.4092  | 0.3929  | 0.3864 \n",
    "| Specificity   | 0.9203  | 0.9273  | 0.9248 \n",
    "| F1 Score      | 0.5016  | 0.4924  | 0.4841 \n",
    "| AUC           | 0.6647  | 0.6601  | 0.6556 \n",
    "\n",
    "Also, when comparing models we should __ALWAYS__ assess the number of explanatory variables used to train the model. As we've learned, relatively less complex models are generally preferable to relatively more complex models, particularly if the difference in their performance is marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Proposal Comments\n",
    "\n",
    "### At least one of your research questions should be indicative of the need to construct a predictive model\n",
    "\n",
    "Research questions that are predicated in identifying \"relationships\" between variables or trying to determine the \"effects\" of one variable on another are not indicative of a need to construct a predictive model. Remember: predictive models __provide estimated values for the given response variable__ relative to the explanatory variables at hand. \n",
    "\n",
    "Questions that are merely interested in \"relationships\" and \"effects\" can readily be answered using a variety of basic EDA tools, e.g., correlation metrics, chi^2 tests, visualizations, etc. \n",
    "\n",
    "Therefore, if your research questions are not indicative of the need for the estimation of a response variable relative to a set of explanatory variables, you should either re-think your final project OR formulate a research question that is __clearly__ indicative of a need for the use of a predictive model.\n",
    "\n",
    "\n",
    "### Every Final Project is required to include the implementation of an ensemble model\n",
    "\n",
    "__Implementation of an ensemble model is NOT \"optional\"__.\n",
    "\n",
    "\"__Explain your plan for combining the output of your three individual models into a single ensemble model__\".\n",
    "\n",
    "Please refer to the __Module 5__ assigned readings if you are still unclear on what an __ensemble model__ is or how to approach the development of an effective ensemble algorithm.\n",
    "\n",
    "\n",
    "### The predictive models you construct should be compared on the basis of a VARIETY of metrics\n",
    "\n",
    "As should be obvious at this point in the course, in \"real world\" data science work practitioners will typically examine a variety of metrics when comparing models rather than relying on a single metric. Model performance metrics were covered in detail in __Module 5__.\n",
    "\n",
    "If you are planning to model a __categorical response variable__, your models should be measured using metrics such as accuracy, precision, recall, specificity, F1 score, AUC score, AIC, BIC, log likelihood, etc.\n",
    "\n",
    "If you are planning to model a __numeric response variable__, your models should be measured using metrics such as AIC, BIC, AUC, RMSE, etc.\n",
    "\n",
    "\n",
    "### Make sure that what you are proposing is realistic relative to the time available\n",
    "\n",
    "A key component of success in any professional environment is knowing how to properly set expectations regarding the work you have committed to delivering relative to any resource + time limitations. For your Final Project, what you are proposing should be legitimately challenging while being \"completable\" within the time you have available to dedicate to the project. \n",
    "\n",
    "Your proposed work should be structured in such a way as to limit the possibility of your not being able to complete your proposed work prior to the Final Project deadline / due date.\n",
    "\n",
    "\n",
    "### Proposals can be revised after they are approved, subject to further review + approval\n",
    "\n",
    "In the real world, proposed projects often need to be re-worked or adjusted due to the discovery of previously unforeseen challenges. If you encounter an insurmountable challenge in your Final Project work, revised your proposal to address the challenge (while ensuring it still meets all of the requirements outlined in the Final Project Guidelines document) and resubmit it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "\n",
    "### What is Clustering ?\n",
    "\n",
    "__Clustering__ (also known as “cluster analysis”) is a data science methodology used for __separating unlabeled data observations into groupings__ wherein the members of each group are more similar to one another than they are to members of other groups. \n",
    "\n",
    "\n",
    "What is \"__unlabeled data__\"? Data for which we have no predefined groupings or meaningful labels, e.g., a collection of photos that have not been categorized according to their content. For data science purposes, we can think of unlabeled data as lacking a defined \"labeling\" variable that clearly delineates observations based on the content of their attributes. For example, for a collection of photos, we could label them based on their content, e.g., \"these photos contain images of animals; these other images contain images of kitchen utensils;\" etc.\n",
    "\n",
    "\n",
    "### Why Use Clustering ?\n",
    "\n",
    "Grouping similar observations together can __give us insight into underlying patterns__ present within those different groupings. Some examples:\n",
    "\n",
    "- Customer segmentation\n",
    "\n",
    "\n",
    "- Grouping documents based on similarity of topics/content\n",
    "\n",
    "\n",
    "- Grouping similar digital images together based on similarity\n",
    "\n",
    "\n",
    "- Outlier detection (e.g., detecting credit card fraud)\n",
    "\n",
    "\n",
    "- etc.\n",
    "\n",
    "Clustering can also be used for purposes of __reducing the dimensionality of complex data sets__. For example, after identifying suitable clusters within a data set we could add a single new attribute indicative of the new groupings and drop some larger number of relatively \"uninformative\" attributes for purposes of model building.\n",
    "\n",
    "\n",
    "### Should Data be Standardized for Clustering ?\n",
    "\n",
    "In general, data is usually __standardized__ prior to the application of a clustering algorithm. Data standardization will ensure that all of the features within a data set are of similar scale. However, standardization can at times result in the removal of useful predictive information from a data set. Therefore, the use of your empirical skills may be prudent, i.e, it may be useful to construct models with and without standardization and then assess their bias + variance and then choose the approach that best balances the bias vs. variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "__K-means clustering__ is an iterative algorithm that will separate a set of __unlabeled data__ of n observations into a predefined number (“K”) of non-overlapping groupings by minimizing the variance of the items within each of the K groupings. \n",
    "\n",
    "\n",
    "K-means clustering can be effective when applied to very large data sets since __its time complexity is linear__\n",
    "\n",
    "\n",
    "### How it Works\n",
    "\n",
    "- Define the number of clusters to be created (__K__)\n",
    "\n",
    "\n",
    "- The number of clusters \"K\" defines the number of randomly placed cluster  __centroids__ to be used for grouping your data\n",
    "\n",
    "\n",
    "- Data points are allocated to centroids / groupings through an iterative process that seeks to minimize the \"within cluster\" sum of squares for all clusters. __Quite often a Euclidean distance metric is used__ to determine the grouping assignment for a given observation.\n",
    "\n",
    "\n",
    "- __Adjust the positions of the centroids__ relative to the data points that have been assigned to them so that the centroids remain at the center of their respective groupings.\n",
    "\n",
    "\n",
    "- The algorithm continues to iterate until either a predefined number of iterations has been applied __OR__ the centroids have achieved stability (i.e., their positions / values no longer vary, which indicates that the algorithm has, in fact, minimized the \"within cluster\" sum of squares)\n",
    "\n",
    "### How to Choose K\n",
    "\n",
    "- __Consult a Subject Matter Expert / Develop Domain Knowledge__: Development of relevant domain knowledge can serve as the basis for deciding upon how many groupings to impose on an unlabeled data set.\n",
    "\n",
    "\n",
    "- __Inertia__: The sum of squared distances between each data point in a cluster and the cluster's centroid is known as __inertia__.  We can use inertia to help us select a value for K. How? execute a series of K-means clustering models using increasing values for K. Calculate the inertia for each model and then plot inertia vs. K in a 2-D graph. An example from the assigned readings (Figure 9-8): https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb. Look for a \"bend\" in the curve: where you see a clear \"bend\" the corresponding value of __K__ is relatively likely to be an effective choice for use in your model. \n",
    "\n",
    "\n",
    "- __Silhouette Score__: Silhouette Score = $ (b-a) / \\max(a, b)$ where $a$ is the mean distance to other observations within the same cluster and $b$ is the mean distance to the observations within the next-nearest cluster. Calculate the mean of this score across all clusters for a variety of values of K  and choose the  K  having a relatively high silhouette score  __AND__ a relatively low value for __K__. An example from the assigned readings (Figure 9-9) : https://github.com/ageron/handson-ml2/blob/master/09_unsupervised_learning.ipynb. \n",
    "\n",
    "__How to interpret a silhouette score?__\n",
    "Silhouette scores will range in value from $(-1 <= score <= 1)$ \n",
    "\n",
    "- A value of $-1$ indicates that an observation has been poorly classified (i.e., it likely belongs in a different cluster).\n",
    "\n",
    "\n",
    "- A value of $0$ indicates that an observation might be reasonably signed to either of its two nearest clusters\n",
    "\n",
    "\n",
    "- A value of $1$ indicates that an observation has been perfectly classified.\n",
    "\n",
    "See the table at the bottom of the following article for a guide on how to interpret other fractional silhouette score values: https://web.archive.org/web/20111002220803/http://www.unesco.org:80/webworld/idams/advguide/Chapt7_1_1.htm\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- __Linear time complexity__ => can be applied to very large data sets when needed\n",
    "\n",
    "\n",
    "- Relatively easy to understand + implement\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- __Not 100% reproducible__ due to the need to have random placement of centroids at start of algorithm. Random placement of the centroids can lead to the creation of clusters whose characteristics vary significantly from those of the clusters generated by a different instance of K-means clustering for the same data set.\n",
    "\n",
    "\n",
    "- Slight variations in feature values can cause a clustering model to have high variance\n",
    "\n",
    "\n",
    "- Clusters are assumed to be spherical in shape + relatively evenly sized. These assumptions may not be applicable to many data sets.\n",
    "\n",
    "\n",
    "### How to Implement K-Means Clustering in Python\n",
    "\n",
    "The __scikit-learn__ library provides a robust pre-built K-means clustering function: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "\n",
    "A simple example from the assigned readings: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "\n",
    "### How it Works\n",
    "\n",
    "__Hierarchical Clustering__ (a.k.a., \"Agglomerative Clustering\") treats each individual data observation as a distinct cluster. Larger clusters are created by iteratively combining relatively proximal clusters until we have finally combined all data observations into a single cluster. This ultimate all-encompassing cluster can be thought of as representing the root of a binary tree. \n",
    "\n",
    "__Step 1__: Assign each data point to its own cluster.\n",
    "\n",
    "\n",
    "__Step 2__: Using the desired __linkage function__, identify the pair of clusters that lie closest to one another. Combine those two clusters into a new, single cluster.\n",
    "\n",
    "\n",
    "__Step 3__: Repeat Step 2 until all clusters have been combined into a single cluster\n",
    "\n",
    "\n",
    "By contrast, in __divisive clustering__ we start with all observations grouped together within a single cluster and then iteratively work to create sub-clusters of relatively similar observations. \n",
    "\n",
    "\n",
    "###  What is a _Linkage Function_ ?\n",
    "\n",
    "A __Linkage Function__ is simply a way of determining the distance between sets of observations (a.k.a., \"clusters\") as a function of the pairwise distances between those sets of observations. There are several different linkage methods typically used with hierarchical clustering algorithms. The methods differ from one another by the way in which they define \"proximity\" between clusters. \n",
    "\n",
    "- __Simple Linkage__ : Also known as __single linkage__; Similar to \"nearest neighbor\" distance; the minimum distance between two clusters\n",
    "\n",
    "\n",
    "- __Average Linkage__: Also known as __centroid linkage__; the distance between the centroids of two clusters\n",
    "\n",
    "\n",
    "- __Complete Linkage__: The largest distance between observations within a pair of clusters\n",
    "\n",
    "\n",
    "A more detailed discussion of linkage functions can be found here: https://www.geeksforgeeks.org/ml-types-of-linkages-in-clustering/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How Many Clusters Have Been Found ? \n",
    "\n",
    "Generate a __Dendrogram__. In a dendrogram, the heights of the vertical lines indicate how similar the underlying clusters are to one another, so the shorter the height, the more similar the two underlying clusters are. \n",
    "\n",
    "To help us identify the \"best\" number of final clusters to utilize in a hierarchical cluster, we can \"draw\" a horizontal line across the __tallest__ branches (i.e., vertical lines) of the \"tree\" while ensuring that none of the underlying clusters overlap with one another. \n",
    "\n",
    "In this example we have three clusters: https://github.com/mattharrison/ml_pocket_reference/blob/master/ch18.ipynb\n",
    "\n",
    "\n",
    "while in this one we have four: https://towardsdatascience.com/clustering-unsupervised-learning-788b215b074b\n",
    "\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- __100% reproducible__: Unlike K-means clustering, hierarchical clustering does not rely on the random placement of centroids.\n",
    "\n",
    "\n",
    "- Easily understandable algorithm\n",
    "\n",
    "\n",
    "- Dendrograms provide a way to interpet the results of the process\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "- __Quadratic time complexity__: Hierarchical clustering __is computationally expensive, having a quadratic time complexity__, and as such __is inappropriate for use with very large data sets__.\n",
    "\n",
    "\n",
    "- Dendrograms can be very difficult to interpret and as a result are easily misinterpreted.\n",
    "\n",
    "\n",
    "- Once an observation has been assigned to a particular cluster, it will remain there, i.e., if the result of the process turns out to be sub-optimal, there is no way to \"backtrack\" and re-arrange the underlying early-stage agglomerations that are formed as the algorithm works to combine all observations into the single all-inclusive cluster that is required at the top of the hierarchy.\n",
    "\n",
    "\n",
    "### How to Implement Hierarchical Clustering in Python ?\n",
    "\n",
    "The __scikit-learn__ library provides a robust pre-built hierarchical clustering function: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html\n",
    "\n",
    "\n",
    "The __scipy__ library also provides a hierarchical clustering function, including a useful Dendrogram generation feature: https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html\n",
    "\n",
    "\n",
    "Examples of both are provided in the assigned MLPR readings: https://github.com/mattharrison/ml_pocket_reference/blob/master/ch18.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Guidelines / Requirements"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
